/**
 * Utility for interacting with local/remote LLM engine APIs (Ollama, LM Studio, etc.)
 * based on OpenAI-compatible or provider-specific endpoints.
 */

export interface LLMModel {
  id: string;
  name?: string;
  object?: string;
  owned_by?: string;
}

export async function testConnection(baseUrl: string): Promise<boolean> {
  try {
    // Try multiple common endpoints if the base one fails
    const endpoints = [`${baseUrl}/models`, `${baseUrl}/v1/models`, `${baseUrl}/api/v1/models`].map(url => url.replace(/([^:])\/\//g, '$1/'));
    
    for (const url of endpoints) {
      try {
        const response = await fetch(url, {
          method: 'GET',
          headers: { 'Content-Type': 'application/json' },
          mode: 'cors',
          signal: AbortSignal.timeout(5000)
        });
        if (response.ok) return true;
      } catch (e) {
        continue;
      }
    }
    return false;
  } catch (error) {
    console.error('Connection test failed:', error);
    return false;
  }
}

export async function fetchModels(baseUrl: string): Promise<LLMModel[]> {
  const endpoints = [`${baseUrl}/models`, `${baseUrl}/v1/models`, `${baseUrl}/api/v1/models`].map(url => url.replace(/([^:])\/\//g, '$1/'));
  
  for (const url of endpoints) {
    try {
      const response = await fetch(url, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        mode: 'cors',
        signal: AbortSignal.timeout(5000)
      });
      
      if (!response.ok) continue;
      
      const data = await response.json();
      
      // Handle different formats
      // 1. OpenAI format: { data: [...] }
      if (data.data && Array.isArray(data.data)) {
        return data.data.map((m: any) => ({ id: m.id, name: m.id }));
      }
      // 2. Ollama format: { models: [...] }
      if (data.models && Array.isArray(data.models)) {
        return data.models.map((m: any) => ({ id: m.name, name: m.name }));
      }
      // 3. Raw array
      if (Array.isArray(data)) {
        return data.map((m: any) => ({ id: typeof m === 'string' ? m : (m.id || m.name), name: typeof m === 'string' ? m : (m.name || m.id) }));
      }
    } catch (error) {
      console.error(`Failed to fetch models from ${url}:`, error);
    }
  }
  return [];
}

export async function callChatCompletion(baseUrl: string, modelId: string, messages: any[], settings: any) {
  // Normalize the endpoint
  let chatUrl = `${baseUrl}/chat/completions`;
  if (baseUrl.includes('/v1')) {
    chatUrl = `${baseUrl}/chat/completions`.replace(/([^:])\/\//g, '$1/');
  } else if (!baseUrl.endsWith('/v1') && !baseUrl.includes('/api/')) {
    chatUrl = `${baseUrl}/v1/chat/completions`.replace(/([^:])\/\//g, '$1/');
  }

  try {
    const response = await fetch(chatUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelId,
        messages,
        temperature: settings.temperature,
        top_p: settings.topP,
        max_tokens: settings.maxTokens,
        stream: false
      }),
      mode: 'cors'
    });

    if (!response.ok) {
      const errorData = await response.text();
      throw new Error(`Engine Error (${response.status}): ${errorData}`);
    }

    const data = await response.json();
    return data.choices?.[0]?.message?.content || "No response generated by the engine.";
  } catch (error: any) {
    console.error('Chat Completion failed:', error);
    throw error;
  }
}
